{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557eaea6",
   "metadata": {},
   "source": [
    "# Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c129d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76019a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18cc31c",
   "metadata": {},
   "source": [
    "# Importing and reading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d7b644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "f = open('C:/Users/user/OneDrive/Desktop/chatbot.txt', 'r', errors = 'ignore')\n",
    "raw_doc = f.read()\n",
    "raw_doc = raw_doc.lower() # Coverts text to lowercase\n",
    "nltk.download('punkt') # Using the Punkt tokenizer\n",
    "nltk.download('wordnet') # Using the WordNet dictionary\n",
    "sentence_tokens = nltk.sent_tokenize(raw_doc) # Converts our doc to list of sentences\n",
    "word_tokens = nltk.word_tokenize(raw_doc) # Converts our doc to list of words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d4fc9e",
   "metadata": {},
   "source": [
    "# Example of sentence tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5edb590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data science definition:-\\n- data science is an interdisciplinary field that combines domain expertise, programming skills, and statistical knowledge to extract meaningful insights and knowledge from data.', 'it encompasses various techniques, algorithms, and tools to analyze and visualize data.']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_tokens[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeefc552",
   "metadata": {},
   "source": [
    "# Example of word tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe99aef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'science']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokens[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b442d0",
   "metadata": {},
   "source": [
    "#  Text  preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd67c92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "# WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d96f2",
   "metadata": {},
   "source": [
    "# Defining the greeting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf8ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREET_INPUTS = {\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\", \"good day\", \"howdy\", \"salutations\", \"hola\", \"hey there\", \"hello there\"}\n",
    "GREET_RESPONSES = [\"Hi!\", \"Hey!\", \"Hi there!\", \"Hello!\", \"I am glad! You are talking to me.\", \"Greetings!\", \"Hey, how can I assist you?\", \"Hello! How can I help you today?\", \"Hey, nice to see you!\", \"What's up?\"]\n",
    "\n",
    "def greet(sentence):\n",
    "    \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREET_INPUTS:\n",
    "            return random.choice(GREET_RESPONSES)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83608396",
   "metadata": {},
   "source": [
    "# Defining a function for handling unknown inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72805d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_unknown():\n",
    "    \n",
    "    unknown_responses = [\"I'm not sure I understand. Can you please rephrase?\", \n",
    "                         \"I didn't catch that. Could you please clarify?\", \n",
    "                         \"I'm sorry, I'm not sure what you mean. Can you try rephrasing that?\", \n",
    "                         \"I'm having trouble understanding. Could you please say it another way?\"]\n",
    "    \n",
    "    return random.choice(unknown_responses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff7ef4d",
   "metadata": {},
   "source": [
    "# Response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b637407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # Tf -> Term frequency, idf -> Inverse document frequency\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcd71552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    \n",
    "    robo1_response = \"\"\n",
    "    TfidfVec = TfidfVectorizer(tokenizer = LemNormalize, stop_words = \"english\")\n",
    "    tfidf = TfidfVec.fit_transform(sentence_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    \n",
    "    if(req_tfidf == 0):\n",
    "        robo1_response = robo1_response + handle_unknown()\n",
    "        return robo1_response\n",
    "    \n",
    "    else:\n",
    "        # Consider multiple sentences for a more detailed response\n",
    "        start_idx = idx  \n",
    "        end_idx = min(len(sentence_tokens), idx + 2)  # Consider two sentences after the selected sentence\n",
    "        robo1_response = robo1_response + ' '.join(sentence_tokens[start_idx:end_idx])\n",
    "        return robo1_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c2ea9",
   "metadata": {},
   "source": [
    "# Defining conversation start/end protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33c5180b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT: My name is Jarvis. Let's have a conversation! Also, if you want to exit any time, just type Bye!\n",
      "User: Hi\n",
      "BOT: Hey, nice to see you!\n",
      "User: Data science definition\n",
      "BOT: data science definition:-\n",
      "- data science is an interdisciplinary field that combines domain expertise, programming skills, and statistical knowledge to extract meaningful insights and knowledge from data. it encompasses various techniques, algorithms, and tools to analyze and visualize data.\n",
      "User: Importance of data science\n",
      "BOT: importance of data science:-\n",
      "- data science plays a crucial role in today's data-driven world. it helps businesses make informed decisions, improve products and services, and gain a competitive edge.\n",
      "User: Challenges in data science\n",
      "BOT: challenges in data science:-\n",
      "- data quality: ensuring the accuracy and reliability of data is a persistent challenge in data science. - ethical considerations: handling sensitive or private data requires careful ethical considerations and compliance with regulations.\n",
      "User: Tools used for data science\n",
      "BOT: tools used for data science:-\n",
      "- python: a versatile programming language with numerous libraries for data manipulation, analysis, and visualization. - r: a statistical programming language known for its powerful data analysis capabilities.\n",
      "User: Applications in data science\n",
      "BOT: applications of data science:-\n",
      "- business intelligence: using data to make informed business decisions and gain a competitive edge. - healthcare analytics: analyzing patient data to improve treatment outcomes and healthcare delivery.\n",
      "User: ueofjdlghqoeyhkhasf\n",
      "BOT: I'm not sure I understand. Can you please rephrase?\n",
      "User: Bye\n",
      "BOT: Goodbye! Take care.\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "print(\"BOT: My name is Jarvis. Let's have a conversation! Also, if you want to exit any time, just type Bye!\")\n",
    "\n",
    "while flag:\n",
    "    user_response = input(\"User: \")\n",
    "    user_response = user_response.lower()\n",
    "\n",
    "    if (user_response != 'bye'):\n",
    "        if (user_response == 'thanks' or user_response == 'thank you'):\n",
    "            flag = False\n",
    "            print(\"BOT: You are welcome...\")\n",
    "\n",
    "        else:\n",
    "          if(greet(user_response) != None):\n",
    "            print(\"BOT: \" + greet(user_response))\n",
    "\n",
    "          else:\n",
    "              sentence_tokens.append(user_response)\n",
    "              word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
    "              final_words = list(set(word_tokens))\n",
    "              print(\"BOT: \" + response(user_response))\n",
    "              sentence_tokens.remove(user_response)\n",
    "\n",
    "    else:\n",
    "        flag = False\n",
    "        print(\"BOT: Goodbye! Take care.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
